import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
import time

# å¯¼å…¥å„ç§æ¨¡å‹
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, 
                               AdaBoostRegressor, ExtraTreesRegressor)
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor

# æ–‡ä»¶æ“ä½œ
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# =================================================================
# ğŸš€ åŠ é€Ÿé…ç½®åŒºåŸŸ - å¯æ ¹æ®éœ€è¦è°ƒæ•´
# =================================================================
USE_SAMPLING = True          # æ˜¯å¦ä½¿ç”¨æ•°æ®é‡‡æ ·ï¼ˆå¤§å¹…åŠ é€Ÿï¼‰
SAMPLE_SIZE = 100000         # é‡‡æ ·æ•°æ®é‡ï¼ˆä¾‹å¦‚ä»65ä¸‡é‡‡æ ·åˆ°10ä¸‡ï¼‰
SAMPLE_RATIO = None          # æˆ–ä½¿ç”¨æ¯”ä¾‹é‡‡æ ·ï¼ˆä¾‹å¦‚ 0.3 è¡¨ç¤º30%ï¼‰

ENABLE_TUNING = True         # æ˜¯å¦å¯ç”¨è‡ªåŠ¨è°ƒå‚
TOP_N_MODELS = 3             # é€‰æ‹©å‰Nä¸ªæ¨¡å‹è¿›è¡Œè°ƒå‚
CV_FOLDS = 3                 # äº¤å‰éªŒè¯æŠ˜æ•°
PATIENCE = 3                 # å®¹å¿å¤šå°‘æ¬¡ä¸æå‡ååœæ­¢ï¼ˆæ—©åœç­–ç•¥ï¼‰
MIN_IMPROVEMENT = 0.0001     # æœ€å°æå‡é˜ˆå€¼ï¼ˆå°äºæ­¤å€¼è§†ä¸ºæ— æå‡ï¼‰

# é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹
MODELS_TO_TEST = {
    'Decision Tree': True,
    'Random Forest': True,
    'Extra Trees': True,
    'Gradient Boosting': True,
    'HistGradient Boosting': True,
    'AdaBoost': True,
}

# =================================================================

# --- å‡†å¤‡å·¥ä½œï¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å¤¹ ---
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_dir = f'/Users/vancefeng/Desktop/ords/AML/spotify_analysis_project/feature_popularity_analysis/results/experiment_{timestamp}'
os.makedirs(output_dir, exist_ok=True)
print(f"åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {output_dir}")
print(f"\nâš™ï¸  é…ç½®:")
print(f"  - æ•°æ®é‡‡æ ·: {'å¯ç”¨' if USE_SAMPLING else 'ç¦ç”¨'}")
if USE_SAMPLING:
    print(f"  - é‡‡æ ·å¤§å°: {SAMPLE_SIZE if SAMPLE_SIZE else f'{SAMPLE_RATIO*100:.0f}%'}")
print(f"  - æ™ºèƒ½è°ƒå‚: {'å¯ç”¨' if ENABLE_TUNING else 'ç¦ç”¨'}")
if ENABLE_TUNING:
    print(f"  - è°ƒå‚æ¨¡å‹æ•°: Top {TOP_N_MODELS}")
    print(f"  - äº¤å‰éªŒè¯: {CV_FOLDS} æŠ˜")
    print(f"  - æ—©åœç­–ç•¥: {PATIENCE} æ¬¡ä¸æå‡åˆ™åœæ­¢")
print()

# --- ä¸»ä»£ç å¼€å§‹ ---

# -----------------------------------------------------------------
# 1. Load the ORIGINAL data
# -----------------------------------------------------------------
file_name = '/Users/vancefeng/Desktop/ords/AML/spotify_analysis_project/data/data_with_famous_artist.csv'
print(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_name}")
start_time = time.time()
df = pd.read_csv(file_name, delimiter=';')
load_time = time.time() - start_time
print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ! Shape: {df.shape} (è€—æ—¶: {load_time:.2f}ç§’)\n")

# -----------------------------------------------------------------
# ğŸš€ åŠ é€Ÿç­–ç•¥1: æ•°æ®é‡‡æ ·
# -----------------------------------------------------------------
if USE_SAMPLING:
    original_size = len(df)
    if SAMPLE_SIZE:
        sample_n = min(SAMPLE_SIZE, original_size)
        df = df.sample(n=sample_n, random_state=42).reset_index(drop=True)
    elif SAMPLE_RATIO:
        df = df.sample(frac=SAMPLE_RATIO, random_state=42).reset_index(drop=True)
    
    print(f"ğŸš€ æ•°æ®é‡‡æ ·: {original_size} -> {len(df)} è¡Œ (å‡å°‘ {(1-len(df)/original_size)*100:.1f}%)")
    print(f"   é¢„è®¡è®­ç»ƒé€Ÿåº¦æå‡: {(original_size/len(df)):.1f}x\n")

# -----------------------------------------------------------------
# 2. ç‰¹å¾å·¥ç¨‹ (Feature Engineering)
# -----------------------------------------------------------------
print("--- 2. ç‰¹å¾å·¥ç¨‹ ---")
df['Mood_Score'] = df['Valence'] + df['Energy']
df['Acoustic_vs_Electronic'] = df['Acousticness'] - df['Instrumentalness']
print("å·²åˆ›å»ºæ–°ç‰¹å¾: 'Mood_Score' å’Œ 'Acoustic_vs_Electronic'")

# -----------------------------------------------------------------
# 3. å®šä¹‰ç‰¹å¾ (X) å’Œç›®æ ‡ (y) - ä½¿ç”¨ Famous_Artist æ›¿ä»£ Artist_Canon
# -----------------------------------------------------------------
print("\n--- 3. å®šä¹‰ç‰¹å¾ä¸ç›®æ ‡ ---")

numerical_features = [
    'Danceability', 'Energy', 'Loudness', 'Speechiness', 
    'Acousticness', 'Instrumentalness', 'Valence',
    'Mood_Score', 'Acoustic_vs_Electronic'
]

# âœ… ä½¿ç”¨ Famous_Artist æ›¿ä»£ Artist_Canon
categorical_features = ['Continent', 'Nationality', 'Famous_Artist']

X = df[numerical_features + categorical_features]
y = df['Pop_points_total']

print(f"æ•°å€¼ç‰¹å¾: {len(numerical_features)} ä¸ª")
print(f"åˆ†ç±»ç‰¹å¾: {len(categorical_features)} ä¸ª ({', '.join(categorical_features)})")
print(f"âœ“ ä½¿ç”¨'Famous_Artist'ç‰¹å¾ (Top 100è‘—åæ­Œæ‰‹)")
print(f"  ç‹¬ç‰¹çš„Famous_Artistæ•°é‡: {df['Famous_Artist'].nunique()}")

# å¤„ç† y ä¸­çš„ NaNs
if y.isnull().any():
    nan_indices = y.index[y.isnull()]
    X = X.drop(index=nan_indices).reset_index(drop=True)
    y = y.drop(index=nan_indices).reset_index(drop=True)

print(f"\nShape of X (features): {X.shape}")
print(f"Shape of y (target): {y.shape}")

# -----------------------------------------------------------------
# 4. æ‹†åˆ†æ•°æ®
# -----------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Train set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}")

# -----------------------------------------------------------------
# 5. åˆ›å»ºé¢„å¤„ç† Pipeline
# -----------------------------------------------------------------

numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough'
)

# -----------------------------------------------------------------
# 6. ç¬¬ä¸€é˜¶æ®µï¼šåŸºå‡†æ¨¡å‹æµ‹è¯•
# -----------------------------------------------------------------
print("\n" + "="*60)
print("ç¬¬ä¸€é˜¶æ®µï¼šåŸºå‡†æ¨¡å‹å¿«é€Ÿè¯„ä¼°")
print("="*60)

all_baseline_models = {
    'Decision Tree': {
        'model': DecisionTreeRegressor(random_state=42),
        'needs_dense': False
    },
    'Random Forest': {
        'model': RandomForestRegressor(
            random_state=42, 
            n_estimators=100,
            n_jobs=-1
        ),
        'needs_dense': False
    },
    'Extra Trees': {
        'model': ExtraTreesRegressor(
            random_state=42, 
            n_estimators=100,
            n_jobs=-1
        ),
        'needs_dense': False
    },
    'Gradient Boosting': {
        'model': GradientBoostingRegressor(
            random_state=42, 
            n_estimators=100,
            max_depth=5
        ),
        'needs_dense': False
    },
    'HistGradient Boosting': {
        'model': HistGradientBoostingRegressor(
            random_state=42, 
            max_iter=100
        ),
        'needs_dense': True
    },
    'AdaBoost': {
        'model': AdaBoostRegressor(
            random_state=42, 
            n_estimators=50
        ),
        'needs_dense': False
    },
}

# æ ¹æ®é…ç½®è¿‡æ»¤æ¨¡å‹
baseline_models = {}
models_need_dense = set()

for name, enabled in MODELS_TO_TEST.items():
    if enabled and name in all_baseline_models:
        model_info = all_baseline_models[name]
        baseline_models[name] = model_info['model']
        if model_info['needs_dense']:
            models_need_dense.add(name)

print(f"å·²é€‰æ‹© {len(baseline_models)} ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•")
if models_need_dense:
    print(f"æ³¨æ„: {models_need_dense} éœ€è¦å¯†é›†çŸ©é˜µï¼Œå°†è‡ªåŠ¨è½¬æ¢")

baseline_results = {}
baseline_pipelines = {}
baseline_times = {}

total_start = time.time()

for i, (name, model) in enumerate(baseline_models.items(), 1):
    print(f"\n[{i}/{len(baseline_models)}] Training {name}...")
    
    model_start = time.time()
    
    # å¯¹äºéœ€è¦å¯†é›†çŸ©é˜µçš„æ¨¡å‹ï¼Œåˆ›å»ºç‰¹æ®Šçš„ pipeline
    if name in models_need_dense:
        categorical_transformer_dense = Pipeline(steps=[
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        preprocessor_dense = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numerical_features),
                ('cat', categorical_transformer_dense, categorical_features)
            ],
            remainder='passthrough'
        )
        
        pipe = Pipeline(steps=[
            ('preprocessor', preprocessor_dense),
            ('regressor', model)
        ])
    else:
        pipe = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('regressor', model)
        ])
    
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    
    model_time = time.time() - model_start
    
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    
    baseline_results[name] = {'R2': r2, 'MSE': mse, 'RMSE': rmse}
    baseline_pipelines[name] = pipe
    baseline_times[name] = model_time
    
    print(f"  R2: {r2:.4f} | RMSE: {rmse:.4f} | è€—æ—¶: {model_time:.2f}ç§’")

total_baseline_time = time.time() - total_start

baseline_df = pd.DataFrame(baseline_results).T.sort_values(by='R2', ascending=False)
print("\n--- åŸºå‡†æ¨¡å‹ç»“æœ (æŒ‰ R2 æ’åº) ---")
print(baseline_df.to_markdown(floatfmt=".4f"))
print(f"\nç¬¬ä¸€é˜¶æ®µæ€»è€—æ—¶: {total_baseline_time:.2f}ç§’")

# -----------------------------------------------------------------
# 7. ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½è‡ªé€‚åº”è¶…å‚æ•°è°ƒä¼˜
# -----------------------------------------------------------------

def greedy_tune_parameter(pipe, X_train, y_train, param_name, param_values, cv_folds, patience, min_improvement):
    """
    è´ªå¿ƒæœç´¢å•ä¸ªå‚æ•°çš„æœ€ä½³å€¼ï¼Œç›´åˆ°æ€§èƒ½ä¸å†æå‡
    
    è¿”å›: (æœ€ä½³å€¼, æœ€ä½³åˆ†æ•°, æœç´¢å†å²)
    """
    best_score = -np.inf
    best_value = None
    history = []
    no_improvement_count = 0
    
    print(f"\n  è°ƒä¼˜å‚æ•°: {param_name}")
    print(f"  å€™é€‰å€¼: {param_values}")
    
    for value in param_values:
        # è®¾ç½®å‚æ•°
        pipe.set_params(**{param_name: value})
        
        # äº¤å‰éªŒè¯è¯„ä¼°
        scores = cross_val_score(pipe, X_train, y_train, cv=cv_folds, 
                                scoring='r2', n_jobs=-1)
        mean_score = scores.mean()
        
        history.append({'value': value, 'score': mean_score})
        
        improvement = mean_score - best_score
        
        print(f"    {param_name}={value}: R2={mean_score:.4f} (æå‡: {improvement:+.4f})")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—æå‡
        if improvement > min_improvement:
            best_score = mean_score
            best_value = value
            no_improvement_count = 0
        else:
            no_improvement_count += 1
            
            # æ—©åœï¼šè¿ç»­å¤šæ¬¡æ— æå‡
            if no_improvement_count >= patience:
                print(f"    âš ï¸ è¿ç»­{patience}æ¬¡æ— æ˜¾è‘—æå‡ï¼Œæå‰åœæ­¢æœç´¢")
                break
    
    print(f"  âœ“ æœ€ä½³ {param_name}={best_value} (R2={best_score:.4f})")
    return best_value, best_score, history


tuned_results = {}
tuned_pipelines = {}
best_params_dict = {}
tuning_times = {}
tuning_histories = {}

if ENABLE_TUNING and len(baseline_models) > 0:
    # é€‰æ‹©è¡¨ç°æœ€å¥½çš„å‰Nä¸ªæ¨¡å‹è¿›è¡Œè°ƒå‚
    top_n = min(TOP_N_MODELS, len(baseline_df))
    top_models = baseline_df.head(top_n).index.tolist()
    print(f"\né€‰æ‹©è¡¨ç°æœ€å¥½çš„{top_n}ä¸ªæ¨¡å‹è¿›è¡Œæ™ºèƒ½è°ƒå‚: {top_models}")
    
    print("\n" + "="*60)
    print("ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½è‡ªé€‚åº”è¶…å‚æ•°è°ƒä¼˜")
    print("="*60)
    print(f"ç­–ç•¥: è´ªå¿ƒæœç´¢ + æ—©åœ (patience={PATIENCE}, min_improvement={MIN_IMPROVEMENT})")
    
    # å®šä¹‰å‚æ•°æœç´¢é¡ºåºå’Œå€™é€‰å€¼ï¼ˆä»ç²—åˆ°ç»†ï¼‰
    param_search_configs = {
        'Decision Tree': [
            ('regressor__max_depth', [10, 20, 30, 50, 100, None]),
            ('regressor__min_samples_leaf', [1, 2, 5, 10, 20, 30]),
            ('regressor__min_samples_split', [2, 5, 10, 20, 30]),
        ],
        'Random Forest': [
            ('regressor__n_estimators', [50, 100, 150, 200, 300, 400, 500]),
            ('regressor__max_depth', [10, 15, 20, 25, 30, None]),
            ('regressor__min_samples_leaf', [1, 2, 5, 10, 15, 20]),
        ],
        'Extra Trees': [
            ('regressor__n_estimators', [50, 100, 150, 200, 300, 400, 500]),
            ('regressor__max_depth', [10, 15, 20, 25, 30, None]),
            ('regressor__min_samples_leaf', [1, 2, 5, 10, 15, 20]),
        ],
        'Gradient Boosting': [
            ('regressor__n_estimators', [50, 100, 150, 200, 300, 400]),
            ('regressor__learning_rate', [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]),
            ('regressor__max_depth', [3, 4, 5, 6, 7, 8]),
        ],
        'HistGradient Boosting': [
            ('regressor__max_iter', [50, 100, 150, 200, 300, 400]),
            ('regressor__learning_rate', [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]),
            ('regressor__max_depth', [5, 10, 15, 20, 25, 30, None]),
        ],
        'AdaBoost': [
            ('regressor__n_estimators', [30, 50, 100, 150, 200, 300]),
            ('regressor__learning_rate', [0.01, 0.1, 0.5, 1.0, 1.5, 2.0]),
        ]
    }
    
    tuning_start = time.time()
    
    for i, model_name in enumerate(top_models, 1):
        print(f"\n{'='*60}")
        print(f"[{i}/{len(top_models)}] æ™ºèƒ½è°ƒä¼˜: {model_name}")
        print(f"{'='*60}")
        
        model_tuning_start = time.time()
        
        if model_name not in baseline_models:
            print(f"è­¦å‘Š: {model_name} ä¸åœ¨ baseline_models ä¸­, è·³è¿‡.")
            continue
        
        # è·å–åŸºå‡†æ¨¡å‹å’Œ pipeline
        base_model = baseline_models[model_name]
        
        # åˆ›å»º pipeline
        if model_name in models_need_dense:
            categorical_transformer_dense = Pipeline(steps=[
                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
            ])
            
            preprocessor_for_tuning = ColumnTransformer(
                transformers=[
                    ('num', numeric_transformer, numerical_features),
                    ('cat', categorical_transformer_dense, categorical_features)
                ],
                remainder='passthrough'
            )
        else:
            preprocessor_for_tuning = preprocessor
        
        pipe = Pipeline(steps=[
            ('preprocessor', preprocessor_for_tuning),
            ('regressor', base_model)
        ])
        
        # è·å–å‚æ•°æœç´¢é…ç½®
        param_configs = param_search_configs.get(model_name, [])
        
        if not param_configs:
            print(f"âš ï¸  {model_name} æ²¡æœ‰å®šä¹‰å‚æ•°æœç´¢é…ç½®ï¼Œè·³è¿‡è°ƒä¼˜")
            continue
        
        # é€ä¸ªå‚æ•°è¿›è¡Œè´ªå¿ƒæœç´¢
        best_params = {}
        search_history = {}
        
        for param_name, param_values in param_configs:
            best_value, best_score, history = greedy_tune_parameter(
                pipe, X_train, y_train, param_name, param_values,
                CV_FOLDS, PATIENCE, MIN_IMPROVEMENT
            )
            best_params[param_name] = best_value
            search_history[param_name] = history
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°ç»„åˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        pipe.set_params(**best_params)
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        
        model_tuning_time = time.time() - model_tuning_start
        
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        
        tuned_results[f"{model_name} (Tuned)"] = {'R2': r2, 'MSE': mse, 'RMSE': rmse}
        tuned_pipelines[f"{model_name} (Tuned)"] = pipe
        best_params_dict[model_name] = best_params
        tuning_times[model_name] = model_tuning_time
        tuning_histories[model_name] = search_history
        
        improvement_r2 = r2 - baseline_results[model_name]['R2']
        improvement_rmse = baseline_results[model_name]['RMSE'] - rmse
        
        print(f"\n{'='*60}")
        print(f"âœ“ {model_name} è°ƒä¼˜å®Œæˆ")
        print(f"{'='*60}")
        print(f"æœ€ä½³å‚æ•°ç»„åˆ:")
        for param, value in best_params.items():
            print(f"  {param}: {value}")
        print(f"\næµ‹è¯•é›†æ€§èƒ½:")
        print(f"  R2: {r2:.4f} (æå‡: {improvement_r2:+.4f})")
        print(f"  RMSE: {rmse:.4f} (æ”¹å–„: {improvement_rmse:+.4f})")
        print(f"  è€—æ—¶: {model_tuning_time:.2f}ç§’")
    
    total_tuning_time = time.time() - tuning_start
    print(f"\nç¬¬äºŒé˜¶æ®µæ€»è€—æ—¶: {total_tuning_time:.2f}ç§’")

# -----------------------------------------------------------------
# 8. åˆå¹¶æ‰€æœ‰ç»“æœ
# -----------------------------------------------------------------
print("\n" + "="*60)
print("æœ€ç»ˆç»“æœæ±‡æ€»")
print("="*60)

all_results = {**baseline_results, **tuned_results}
results_df = pd.DataFrame(all_results).T.sort_values(by='R2', ascending=False)

print("\n--- æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯” (æŒ‰ R2 æ’åº) ---")
print(results_df.to_markdown(floatfmt=".4f"))

all_pipelines = {**baseline_pipelines, **tuned_pipelines}

# -----------------------------------------------------------------
# 9. æœ€ä½³æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
# -----------------------------------------------------------------

best_model_name = results_df.index[0]
best_pipeline = all_pipelines[best_model_name]
print(f"\n\n--- æœ€ä½³æ¨¡å‹: '{best_model_name}' ---")

importance_available = False
current_model_name = best_model_name
current_pipeline = best_pipeline

for idx in range(len(results_df)):
    current_model_name = results_df.index[idx]
    if current_model_name not in all_pipelines:
        continue
    current_pipeline = all_pipelines[current_model_name]
    
    if 'regressor' not in current_pipeline.named_steps:
        continue
        
    if hasattr(current_pipeline.named_steps['regressor'], 'feature_importances_'):
        importance_available = True
        break

if importance_available:
    feature_names = current_pipeline.named_steps['preprocessor'].get_feature_names_out()
    importances = current_pipeline.named_steps['regressor'].feature_importances_
    
    if current_model_name != best_model_name:
        print(f"æ³¨æ„: '{best_model_name}' ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§ã€‚")
        print(f"æ”¹ç”¨ '{current_model_name}' çš„ç‰¹å¾é‡è¦æ€§è¿›è¡Œå¯è§†åŒ–ã€‚")
    
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    print(f"\nTotal features after encoding: {len(feature_names)}")
    print("\nTop 15 most important features:")
    print(importance_df.head(15).to_markdown(index=False, floatfmt=".4f"))
    
    # ç»˜å›¾å¹¶ä¿å­˜ä¸º PDF
    plt.figure(figsize=(12, 8))
    sns.barplot(
        data=importance_df.head(15), 
        x='Importance', 
        y='Feature', 
        palette='viridis'
    )
    plt.title(f'Top 15 Feature Importances\n(from {current_model_name})', fontsize=14, fontweight='bold')
    plt.xlabel('Importance Score', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.tight_layout()
    
    pdf_filename = os.path.join(output_dir, 'feature_importances.pdf')
    plt.savefig(pdf_filename, format='pdf', dpi=300)
    print(f"\nâœ“ ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜: {pdf_filename}")
else:
    print(f"\næ— æ³•ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾è¡¨: å€™é€‰æ¨¡å‹å‡ä¸æ”¯æŒ feature_importances_")

# -----------------------------------------------------------------
# 10. ä¿å­˜è¯¦ç»†ç»“æœåˆ° TXT æ–‡ä»¶
# -----------------------------------------------------------------

txt_filename = os.path.join(output_dir, 'model_experiment_results.txt')
with open(txt_filename, 'w', encoding='utf-8') as f:
    f.write("="*70 + "\n")
    f.write("æœºå™¨å­¦ä¹ æ¨¡å‹å®éªŒç»“æœæ±‡æ€» (æ™ºèƒ½è‡ªé€‚åº”è°ƒå‚ç‰ˆ)\n")
    f.write("="*70 + "\n\n")
    f.write(f"å®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"æ•°æ®æ–‡ä»¶: {file_name}\n")
    f.write(f"æ•°æ®å½¢çŠ¶: {df.shape}\n")
    f.write(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}\n")
    f.write(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}\n")
    f.write(f"ç‰¹å¾é…ç½®: ä½¿ç”¨'Famous_Artist'ç‰¹å¾ (Top 100è‘—åæ­Œæ‰‹)\n")
    f.write(f"  Famous_Artistç‹¬ç‰¹å€¼æ•°: {df['Famous_Artist'].nunique()}\n\n")
    
    f.write("é…ç½®:\n")
    f.write(f"  - æ•°æ®é‡‡æ ·: {'å¯ç”¨' if USE_SAMPLING else 'ç¦ç”¨'}\n")
    if USE_SAMPLING:
        f.write(f"  - é‡‡æ ·å¤§å°: {len(df)}\n")
    f.write(f"  - æ™ºèƒ½è°ƒå‚: {'å¯ç”¨' if ENABLE_TUNING else 'ç¦ç”¨'}\n")
    if ENABLE_TUNING:
        f.write(f"  - è°ƒå‚æ¨¡å‹æ•°: Top {TOP_N_MODELS}\n")
        f.write(f"  - æ—©åœç­–ç•¥: patience={PATIENCE}, min_improvement={MIN_IMPROVEMENT}\n")
    f.write("\n")
    
    f.write("-"*70 + "\n")
    f.write("ç¬¬ä¸€é˜¶æ®µ: åŸºå‡†æ¨¡å‹ç»“æœ\n")
    f.write("-"*70 + "\n\n")
    f.write(baseline_df.to_string())
    f.write("\n\nè®­ç»ƒæ—¶é—´:\n")
    for name, t in baseline_times.items():
        f.write(f"  {name}: {t:.2f}ç§’\n")
    f.write("\n")
    
    if ENABLE_TUNING and best_params_dict:
        f.write("-"*70 + "\n")
        f.write("ç¬¬äºŒé˜¶æ®µ: æ™ºèƒ½è°ƒä¼˜ç»“æœ\n")
        f.write("-"*70 + "\n\n")
        for model_name, params in best_params_dict.items():
            f.write(f"\n{model_name}:\n")
            f.write(f"  æœ€ä½³å‚æ•°:\n")
            for param, value in params.items():
                f.write(f"    {param}: {value}\n")
            
            if model_name in tuning_histories:
                f.write(f"\n  å‚æ•°æœç´¢å†å²:\n")
                for param_name, history in tuning_histories[model_name].items():
                    f.write(f"    {param_name}:\n")
                    for record in history:
                        f.write(f"      {record['value']}: R2={record['score']:.4f}\n")
            
            if model_name in tuning_times:
                f.write(f"  è°ƒä¼˜è€—æ—¶: {tuning_times[model_name]:.2f}ç§’\n")
        f.write("\n")
    
    f.write("-"*70 + "\n")
    f.write("æœ€ç»ˆæ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯” (æŒ‰ R2 æ’åº)\n")
    f.write("-"*70 + "\n\n")
    f.write(results_df.to_string())
    f.write("\n\n")
    
    if importance_available:
        f.write("-"*70 + "\n")
        f.write(f"Top 20 ç‰¹å¾é‡è¦æ€§ (æ¥è‡ª {current_model_name})\n")
        f.write("-"*70 + "\n\n")
        f.write(importance_df.head(20).to_string())
    
    f.write("\n\n")
    f.write("="*70 + "\n")
    f.write(f"æœ€ä½³æ¨¡å‹: {best_model_name}\n")
    f.write(f"R2 Score: {results_df.loc[best_model_name, 'R2']:.4f}\n")
    f.write(f"RMSE: {results_df.loc[best_model_name, 'RMSE']:.4f}\n")
    f.write("="*70 + "\n")

# è®¡ç®—æ€»è€—æ—¶
total_time = time.time() - start_time
minutes = int(total_time // 60)
seconds = total_time % 60

print(f"\n{'='*60}")
print(f"--- å®éªŒå®Œæˆ ---")
print(f"{'='*60}")
print(f"æ€»è€—æ—¶: {minutes}åˆ†{seconds:.1f}ç§’")
print(f"è¾“å‡ºæ–‡ä»¶å¤¹: {output_dir}")
print(f"  âœ“ å®éªŒç»“æœæ‘˜è¦: model_experiment_results.txt")
if importance_available:
    print(f"  âœ“ ç‰¹å¾é‡è¦æ€§å›¾: feature_importances.pdf")
print(f"{'='*60}")

# æ˜¾ç¤ºæœ€ç»ˆæ€§èƒ½å¯¹æ¯”
print("\n=== æœ€ç»ˆæ¨¡å‹æ€§èƒ½æ’å (Top 5) ===")
print(results_df.head().to_markdown(floatfmt=".4f"))

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   R2 Score: {results_df.loc[best_model_name, 'R2']:.4f}")
print(f"   RMSE: {results_df.loc[best_model_name, 'RMSE']:.4f}")

# æ˜¾ç¤ºæ€§èƒ½æå‡æ‘˜è¦
if ENABLE_TUNING and best_params_dict:
    print(f"\nğŸ“Š è°ƒä¼˜æ•ˆæœæ‘˜è¦:")
    for model_name in best_params_dict.keys():
        baseline_r2 = baseline_results[model_name]['R2']
        tuned_r2 = tuned_results[f"{model_name} (Tuned)"]['R2']
        improvement = tuned_r2 - baseline_r2
        improvement_pct = (improvement / baseline_r2) * 100
        print(f"   {model_name}: R2æå‡ {improvement:+.4f} ({improvement_pct:+.2f}%)")